import json
import os
import glob
import numpy as np
import multiprocessing
import azure.cognitiveservices.speech as speechsdk
from moviepy.editor import *
from PIL import Image, ImageDraw, ImageFont
from dotenv import load_dotenv

# -------------------------------------------------------------------------
# [ì´ˆê¸° ì„¤ì •] í™˜ê²½ë³€ìˆ˜ ë° í°íŠ¸ ë¡œë“œ
# -------------------------------------------------------------------------
load_dotenv()

# â˜… í°íŠ¸ ê²½ë¡œ ì„¤ì • (Windows: ë§‘ì€ ê³ ë”• / Mac: AppleSDGothicNeo ë“±)
# íŒŒì¼ì´ ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ”ì§€ ê¼­ í™•ì¸í•˜ì„¸ìš”!
FONT_PATH = "C:/Windows/Fonts/malgun.ttf"  
# FONT_PATH = "/System/Library/Fonts/AppleSDGothicNeo.ttc" # Mac ì˜ˆì‹œ

# ìë§‰ ë””ìì¸ ì„¤ì •
SUBTITLE_FONT_SIZE = 45
TITLE_FONT_SIZE = 80
SUBTITLE_COLOR = "white"
SUBTITLE_BG_COLOR = (0, 0, 0, 160) # ë°˜íˆ¬ëª… ê²€ì • ë°•ìŠ¤ (R, G, B, Alpha)

# í•œ í™”ë©´ì— ë³´ì—¬ì¤„ ìµœëŒ€ ê¸€ì ìˆ˜ (ì´ê±¸ ë„˜ìœ¼ë©´ ë‹¤ìŒ ìë§‰ìœ¼ë¡œ ë¶„í• )
MAX_CHARS_PER_SCREEN = 40  

# Azure Speech API í‚¤
SPEECH_KEY = os.getenv("SPEECH_KEY")
SPEECH_REGION = os.getenv("SPEECH_REGION")

# -------------------------------------------------------------------------
# [í•¨ìˆ˜ 1] PILì„ ì´ìš©í•œ í…ìŠ¤íŠ¸ ì´ë¯¸ì§€ ìƒì„± (ì •ë ¬/ì¤„ë°”ê¿ˆ ì™„ë²½ í•´ê²°)
# -------------------------------------------------------------------------
def create_text_clip_pil(text, font_path, font_size, color, bg_color=None, duration=1, size=(1792, 1024), pos='center'):
    """
    MoviePy TextClip ëŒ€ì‹  PILë¡œ í…ìŠ¤íŠ¸ ì´ë¯¸ì§€ë¥¼ ê·¸ë ¤ì„œ ë°˜í™˜í•©ë‹ˆë‹¤.
    - ì¤‘ì•™ ì •ë ¬ ì™„ë²½ ì§€ì›
    - ë°°ê²½ ë°•ìŠ¤ ìë™ í¬ê¸° ì¡°ì ˆ
    - ê¸€ì í…Œë‘ë¦¬(Stroke) ì§€ì›
    """
    W, H = size
    
    # 1. í°íŠ¸ ë¡œë“œ
    try:
        font = ImageFont.truetype(font_path, font_size)
    except OSError:
        print(f"âš ï¸ í°íŠ¸ ë¡œë“œ ì‹¤íŒ¨({font_path}). ê¸°ë³¸ í°íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        font = ImageFont.load_default()
    
    # ë¹ˆ íˆ¬ëª… ì´ë¯¸ì§€ ìƒì„±
    img = Image.new('RGBA', size, (0, 0, 0, 0))
    draw = ImageDraw.Draw(img)

    # 2. ì‹œê°ì  ì¤„ë°”ê¿ˆ (í™”ë©´ ë„ˆë¹„ 85% ë„˜ì–´ê°€ë©´ ê°•ì œ ê°œí–‰)
    max_width_px = W * 0.85
    visual_lines = []
    
    # ì…ë ¥ëœ í…ìŠ¤íŠ¸ê°€ ì´ë¯¸ ì¤„ë°”ê¿ˆì´ ë˜ì–´ ìˆì„ ìˆ˜ë„ ìˆìœ¼ë¯€ë¡œ split('\n') ì²˜ë¦¬
    for paragraph in text.split('\n'):
        words = paragraph.split()
        current_line = []
        
        for word in words:
            # "í˜„ì¬ ì¤„ + ìƒˆ ë‹¨ì–´" ê¸¸ì´ë¥¼ ë¯¸ë¦¬ ì¸¡ì •
            test_line = " ".join(current_line + [word])
            line_w = font.getbbox(test_line)[2] # getbboxì˜ 3ë²ˆì§¸ ê°’ì´ width
            
            if line_w <= max_width_px:
                current_line.append(word)
            else:
                # ë„˜ì¹˜ë©´ í˜„ì¬ ì¤„ ì €ì¥í•˜ê³  ë‹¤ìŒ ì¤„ë¡œ ì´ë™
                visual_lines.append(" ".join(current_line))
                current_line = [word]
        
        if current_line:
            visual_lines.append(" ".join(current_line))

    # 3. í…ìŠ¤íŠ¸ ì „ì²´ ë†’ì´ ë° ì¢Œí‘œ ê³„ì‚°
    # í•œ ì¤„ ë†’ì´ ê³„ì‚° (í•œê¸€ ê¸°ì¤€)
    ascent, descent = font.getmetrics()
    line_height = ascent + descent + 10 # ì—¬ìœ ë¶„ 10px
    total_text_h = line_height * len(visual_lines)
    
    # Y ì¢Œí‘œ ê²°ì •
    if pos == 'center':
        y = (H - total_text_h) / 2
    elif pos == 'bottom':
        y = H - total_text_h - 100 # ë°”ë‹¥ì—ì„œ 100px ìœ„
    else:
        y = 100

    # 4. ë°°ê²½ ë°•ìŠ¤ ê·¸ë¦¬ê¸° (í…ìŠ¤íŠ¸ê°€ ìˆì„ ê²½ìš°ë§Œ)
    if bg_color and text.strip():
        max_line_w = 0
        for line in visual_lines:
            w = font.getbbox(line)[2]
            if w > max_line_w: max_line_w = w
            
        padding = 20
        # ë°•ìŠ¤ ì¢Œí‘œ ê³„ì‚° (ì¤‘ì•™ ì •ë ¬ ê¸°ì¤€)
        bx1 = (W - max_line_w) / 2 - padding
        by1 = y - padding
        bx2 = (W + max_line_w) / 2 + padding
        by2 = y + total_text_h + padding - 5
        
        draw.rectangle([bx1, by1, bx2, by2], fill=bg_color)

    # 5. í…ìŠ¤íŠ¸ ê·¸ë¦¬ê¸° (í…Œë‘ë¦¬ í¬í•¨)
    cur_y = y
    for line in visual_lines:
        w = font.getbbox(line)[2]
        x = (W - w) / 2 # â˜… ìˆ˜ë™ ì¤‘ì•™ ì •ë ¬ ê³„ì‚°
        
        # ê²€ì€ìƒ‰ í…Œë‘ë¦¬ (Stroke) íš¨ê³¼ - 4ë°©í–¥ìœ¼ë¡œ ê·¸ë ¤ì„œ êµ¬í˜„
        stroke_width = 2
        for off_x in range(-stroke_width, stroke_width+1):
            for off_y in range(-stroke_width, stroke_width+1):
                 draw.text((x+off_x, cur_y+off_y), line, font=font, fill="black")
        
        # ë©”ì¸ í…ìŠ¤íŠ¸
        draw.text((x, cur_y), line, font=font, fill=color)
        cur_y += line_height

    # 6. Numpy ë°°ì—´ë¡œ ë³€í™˜í•˜ì—¬ ImageClip ìƒì„±
    return ImageClip(np.array(img)).set_duration(duration)

# -------------------------------------------------------------------------
# [í•¨ìˆ˜ 2] ìë§‰ ì‹œê°„ ë¶„ë°° ë¡œì§ (ì–´ì ˆ ë‹¨ìœ„ ë¶„í• )
# -------------------------------------------------------------------------
def split_subtitle_chunks(text, total_duration, max_chars=40):
    """
    ê¸´ ë¬¸ì¥ì„ ì–´ì ˆ ë‹¨ìœ„ë¡œ ëŠì–´ì„œ max_charsë¥¼ ë„˜ì§€ ì•Šê²Œ ë©ì–´ë¦¬ë¡œ ë‚˜ëˆ”.
    ì‹œê°„ì€ ê¸€ì ìˆ˜ì— ë¹„ë¡€í•˜ì—¬ ë°°ë¶„.
    """
    words = text.split()
    chunks = []
    
    current_chunk_words = []
    current_len = 0
    
    # 1. í…ìŠ¤íŠ¸ ë©ì–´ë¦¬ ë‚˜ëˆ„ê¸°
    for word in words:
        word_len = len(word)
        if current_len + word_len + 1 <= max_chars:
            current_chunk_words.append(word)
            current_len += word_len + 1
        else:
            if current_chunk_words:
                chunks.append(" ".join(current_chunk_words))
            current_chunk_words = [word]
            current_len = word_len + 1
            
    if current_chunk_words:
        chunks.append(" ".join(current_chunk_words))
    
    if not chunks:
        return []

    # 2. ì‹œê°„ ë°°ë¶„ (ê¸€ì ìˆ˜ ë¹„ë¡€)
    total_char_count = sum(len(c.replace(" ", "")) for c in chunks)
    if total_char_count == 0: total_char_count = 1
    
    result = []
    for chunk_text in chunks:
        chunk_len = len(chunk_text.replace(" ", ""))
        chunk_duration = total_duration * (chunk_len / total_char_count)
        
        # ë„ˆë¬´ ì§§ì€ ìë§‰ ë°©ì§€ (ìµœì†Œ 1ì´ˆ ë³´ì¥, ë‹¨ ì „ì²´ ê¸¸ì´ê°€ ì¶©ë¶„í•  ë•Œ)
        if chunk_duration < 1.0 and total_duration > len(chunks):
             chunk_duration = 1.0
             
        result.append({'text': chunk_text, 'duration': chunk_duration})
        
    # ë§ˆì§€ë§‰ ìë§‰ ì‹œê°„ ë³´ì • (ì˜¤ì°¨ ìˆ˜ì •)
    calc_total = sum(r['duration'] for r in result)
    if result:
        result[-1]['duration'] += (total_duration - calc_total)
        
    return result

# -------------------------------------------------------------------------
# [í•¨ìˆ˜ 3] ì œëª© ì˜¤ë””ì˜¤ ìƒì„± (Azure TTS)
# -------------------------------------------------------------------------
def generate_title_audio(text, output_path):
    if os.path.exists(output_path): return True
    try:
        speech_config = speechsdk.SpeechConfig(subscription=SPEECH_KEY, region=SPEECH_REGION)
        speech_config.speech_synthesis_voice_name = "ko-KR-HyunsuMultilingualNeural" # í•´ì„¤ì í†¤
        audio_config = speechsdk.audio.AudioOutputConfig(filename=output_path)
        synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config, audio_config=audio_config)
        result = synthesizer.speak_text_async(text).get()
        return result.reason == speechsdk.ResultReason.SynthesizingAudioCompleted
    except Exception as e:
        print(f"âŒ ì œëª© TTS ì—ëŸ¬: {e}")
        return False

# -------------------------------------------------------------------------
# [ë©”ì¸ ë¡œì§] ë¹„ë””ì˜¤ ìƒì„±
# -------------------------------------------------------------------------
def create_video_for_story(story_data, base_dir="output_assets"):
    title = story_data['title']
    safe_title = "".join([c for c in title if c.isalnum() or c in (' ', '_')]).strip()
    
    story_dir = os.path.join(base_dir, safe_title)
    audio_dir = os.path.join(story_dir, "audio")
    image_dir = os.path.join(story_dir, "images")
    output_video_path = os.path.join(story_dir, f"{safe_title}_final.mp4")
    
    # í•´ìƒë„ ì„¤ì • (ì´ë¯¸ì§€ ìƒì„± ì‚¬ì´ì¦ˆì™€ ë™ì¼í•˜ê²Œ ë§ì¶¤)
    VIDEO_SIZE = (1536, 1024) 

    print(f"ğŸ¬ [ì˜ìƒ í¸ì§‘ ì‹œì‘] '{title}'")

    if not os.path.exists(audio_dir) or not os.path.exists(image_dir):
        print(f"  âŒ ìì‚° í´ë”ê°€ ì—†ì–´ ê±´ë„ˆëœë‹ˆë‹¤.")
        return

    final_clips = []

    # ==========================================
    # 1. ì¸íŠ¸ë¡œ (Intro) ì œì‘
    # ==========================================
    title_audio_path = os.path.join(audio_dir, "00_intro_title.mp3")
    
    # ì œëª© ì˜¤ë””ì˜¤ ìƒì„± ì‹œë„
    has_intro_audio = generate_title_audio(title, title_audio_path)
    
    if has_intro_audio:
        title_audio = AudioFileClip(title_audio_path)
        intro_dur = title_audio.duration + 2.0 # ì—¬ìœ  ì‹œê°„ 2ì´ˆ
        
        # ì œëª© ìë§‰ (ì¤‘ì•™ ì •ë ¬, í˜ì´ë“œì¸ íš¨ê³¼)
        title_clip = create_text_clip_pil(
            title, FONT_PATH, TITLE_FONT_SIZE, "white", 
            duration=intro_dur, size=VIDEO_SIZE, pos='center'
        )
        
        # ê²€ì€ ë°°ê²½
        bg_clip = ColorClip(size=VIDEO_SIZE, color=(0,0,0), duration=intro_dur)
        
        # í•©ì„±
        intro_video = CompositeVideoClip([bg_clip, title_clip]).set_audio(title_audio).fadein(1.5)
        final_clips.append(intro_video)
        print("  âœ… ì¸íŠ¸ë¡œ ìƒì„± ì™„ë£Œ")

    # ==========================================
    # 2. ë³¸ë¬¸ ì”¬(Scene) ë£¨í”„
    # ==========================================
    scenes = story_data.get('scenes', [])
    for scene in scenes:
        scene_num = scene['scene_num']
        scripts = scene['scripts']
        
        print(f"  ğŸï¸ ì¥ë©´ {scene_num} êµ¬ì„± ì¤‘...")

        # ì´ë¯¸ì§€ ë¡œë“œ
        img_filename = f"S{scene_num:02d}.png"
        img_path = os.path.join(image_dir, img_filename)
        if not os.path.exists(img_path):
            print(f"    âš ï¸ ì´ë¯¸ì§€ ì—†ìŒ: {img_filename}")
            continue

        scene_audio_clips = []
        scene_subtitle_clips = []
        current_time = 0 
        
        # ìŠ¤í¬ë¦½íŠ¸(ëŒ€ì‚¬) ë£¨í”„
        for idx, script in enumerate(scripts):
            role = script['role']
            text = script['text']
            
            # ì˜¤ë””ì˜¤ íŒŒì¼ ì°¾ê¸° (íŒŒì¼ëª… íŒ¨í„´ ë§¤ì¹­)
            pattern = os.path.join(audio_dir, f"S{scene_num:02d}_{idx:03d}_{role}_*.mp3")
            matches = glob.glob(pattern)
            
            if not matches: continue
            
            audio_path = matches[0]
            try:
                audio_clip = AudioFileClip(audio_path)
                total_duration = audio_clip.duration
                scene_audio_clips.append(audio_clip)
                
                # â˜… ìë§‰ ë¶„í•  ë° ìƒì„± (í•µì‹¬ ë¡œì§)
                subtitle_chunks = split_subtitle_chunks(text, total_duration, MAX_CHARS_PER_SCREEN)
                
                for chunk in subtitle_chunks:
                    chunk_text = chunk['text']
                    chunk_dur = chunk['duration']
                    
                    # PILë¡œ ìë§‰ ì´ë¯¸ì§€ ìƒì„± (í•˜ë‹¨ ì •ë ¬)
                    txt_clip = create_text_clip_pil(
                        chunk_text, FONT_PATH, SUBTITLE_FONT_SIZE, SUBTITLE_COLOR, 
                        bg_color=SUBTITLE_BG_COLOR, 
                        duration=chunk_dur, 
                        size=VIDEO_SIZE, 
                        pos='bottom'
                    )
                    
                    # ì‹œì‘ ì‹œê°„ ì„¤ì • í›„ ë¦¬ìŠ¤íŠ¸ ì¶”ê°€
                    txt_clip = txt_clip.set_start(current_time)
                    scene_subtitle_clips.append(txt_clip)
                    
                    current_time += chunk_dur
                
            except Exception as e:
                print(f"    âŒ í´ë¦½ ì²˜ë¦¬ ì—ëŸ¬: {e}")

        if not scene_audio_clips: continue

        # ì”¬ í•©ì„± (ì˜¤ë””ì˜¤ ì—°ê²° + ì´ë¯¸ì§€ ë°°ê²½ + ìë§‰ë“¤)
        combined_audio = concatenate_audioclips(scene_audio_clips)
        total_dur = combined_audio.duration + 0.5 # 0.5ì´ˆ ì—¬ìœ 
        
        # ë°°ê²½ ì´ë¯¸ì§€ (í¬ë¡œìŠ¤í˜ì´ë“œ íš¨ê³¼ ì¶”ê°€)
        base_img = ImageClip(img_path).set_duration(total_dur).crossfadein(0.5)
        
        # CompositeVideoClipì€ [ë°°ê²½, ìë§‰1, ìë§‰2...] ìˆœì„œë¡œ ë„£ì–´ì•¼ í•¨
        final_scene = CompositeVideoClip([base_img] + scene_subtitle_clips).set_audio(combined_audio)
        final_clips.append(final_scene)

    # ==========================================
    # 3. ìµœì¢… ë Œë”ë§ (ê³ ì† ëª¨ë“œ)
    # ==========================================
    if final_clips:
        print(f"  ğŸ’¾ ë Œë”ë§ ì‹œì‘... (ì„¤ì •: Ultrafast, Threads=Max)")
        final_video = concatenate_videoclips(final_clips, method="compose")
        
        # CPU ì½”ì–´ ìˆ˜ í™•ì¸
        cpu_count = multiprocessing.cpu_count()
        
        try:
            final_video.write_videofile(
                output_video_path, 
                fps=24, 
                codec='libx264', 
                audio_codec='aac',
                threads=cpu_count,     # ë©€í‹°ì“°ë ˆë”©
                preset='ultrafast',    # ì†ë„ ìµœìš°ì„ 
                ffmpeg_params=['-tune', 'stillimage'] # ì •ì§€ ì˜ìƒ ìµœì í™”
            )
            print(f"ğŸ‰ ì˜ìƒ ì œì‘ ì„±ê³µ! \nğŸ“ ìœ„ì¹˜: {output_video_path}\n")
        except Exception as e:
            print(f"âŒ ë Œë”ë§ ì‹¤íŒ¨: {e}")
    else:
        print("âŒ ìƒì„±í•  í´ë¦½ì´ ì—†ìŠµë‹ˆë‹¤.")

def main(input_file):
    if os.path.exists(input_file):
        with open(input_file, 'r', encoding='utf-8') as f:
            for story in json.load(f):
                create_video_for_story(story)

if __name__ == "__main__":
    main("processed_stories.json")